{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guillermobc\\AppData\\Local\\Temp\\ipykernel_33180\\1938499726.py:10: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pprint import pprint\n",
    "from dotenv import load_dotenv\n",
    "import pathlib\n",
    "# from tqdm import tqdm\n",
    "from tqdm.autonotebook import tqdm\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import hashlib\n",
    "import re\n",
    "\n",
    "import polars as pl\n",
    "from glob import glob\n",
    "\n",
    "from pdfminer.high_level import extract_pages, extract_text\n",
    "from pdfminer.layout import LTTextContainer, LTChar\n",
    "\n",
    "from openai import OpenAI\n",
    "from pinecone import Pinecone\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import tiktoken\n",
    "\n",
    "from pinecone_text.sparse import BM25Encoder\n",
    "\n",
    "\n",
    "root_dir = Path(os.getcwd()).parent.parent\n",
    "sys.path.insert(0, str(root_dir))\n",
    "\n",
    "\n",
    "from src.d01_data.data_izeta import *\n",
    "\n",
    "\n",
    "pl.Config.set_fmt_str_lengths(300)\n",
    "pl.Config.set_tbl_rows(100)\n",
    "pl.Config.set_tbl_cols(20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.d01_data.data import (read_json, create_index_if_not_exists, upsert_vectors_in_batches, json_dump)\n",
    "\n",
    "from src.d00_utils.utils import (get_tokens_len, generate_sparse_vector_in_batches,\n",
    "                                 dict_to_document_boe, metadata_to_uuid)\n",
    "\n",
    "load_dotenv('../../.env')\n",
    "\n",
    "raw_path = root_dir / 'data' / '01_raw'\n",
    "intermediate_path = root_dir / 'data' / '02_intermediate'\n",
    "output_path = root_dir / 'data' / '04_model_output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model used: gpt-4o-mini-2024-07-18\n"
     ]
    }
   ],
   "source": [
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')\n",
    "EMBEDDING_MODEL = os.getenv('EMBEDDING_MODEL')\n",
    "\n",
    "MODEL = os.getenv('OPENAI_MODEL')\n",
    "print(f'Model used: {MODEL}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estructurar_autorizaciones(pdf_path, pdf_name):\n",
    "    \"\"\"\n",
    "    Organizes the content of an authorization PDF into a hierarchical structure based on predefined sections. \n",
    "    The classification is performed using text patterns, element coordinates, and font size analysis.\n",
    "\n",
    "    Parameters:\n",
    "    - pdf_path (str): Path to the PDF file.\n",
    "    - pdf_name (str): Name of the PDF, used to encapsulate the structure in the result.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Hierarchical structure of the PDF content.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define patterns for different levels in the Izeta PDFs\n",
    "    section_1_pattern = re.compile(r'Tipo de autorización')\n",
    "    section_2_pattern = re.compile(r'Normativa básica')\n",
    "    section_3_pattern = re.compile(r'Requisitos')\n",
    "    section_4_pattern = re.compile(r'Documentación exigible')\n",
    "    section_5_pattern = re.compile(r'Procedimiento')\n",
    "\n",
    "    estructura = {}\n",
    "    estructura_final = {}\n",
    "    current_section = 'Sin Seccion'\n",
    "    indice_flag = True  # Flag to determine if we are inside the table of contents\n",
    "\n",
    "    \n",
    "    for page_layout in extract_pages(pdf_path):\n",
    "        for element in page_layout:\n",
    "            if isinstance(element, LTTextContainer):\n",
    "                text = element.get_text().strip()\n",
    "                x0, y0, x1, y1 = element.bbox\n",
    "\n",
    "                # Create a set with the font size of eache element text\n",
    "                font_sizes = set()\n",
    "\n",
    "                for text_line in element:\n",
    "                    if isinstance(text_line, LTTextContainer):\n",
    "                        for character in text_line:\n",
    "                            if isinstance(character, LTChar):\n",
    "                                font_sizes.add(character.size)\n",
    "\n",
    "                # Skip page numbers\n",
    "                if not font_sizes or (max(list(font_sizes)) < 8):\n",
    "                    continue\n",
    "\n",
    "                # Detect the end of the table of contents\n",
    "                if (max(list(font_sizes)) > 17):\n",
    "                    indice_flag = False\n",
    "                    final_name = text.replace('\\n', ' ')\n",
    "\n",
    "                # Skip lines that belong to the table of contents\n",
    "                if indice_flag:\n",
    "                    continue\n",
    "\n",
    "                # Skip irrelevant text\n",
    "                if (x0 < 52) or (x0 > 550):\n",
    "                    continue\n",
    "\n",
    "                # Filter only text that corresponds to numbered sections\n",
    "                if (re.match(section_1_pattern, text) or re.match(section_2_pattern, text) or re.match(section_3_pattern, text) or re.match(section_4_pattern, text) or re.match(section_5_pattern, text)) and ((x0 < 63)):  # Main section\n",
    "                    current_section = text\n",
    "\n",
    "                # If it's not a new section, subsection, or sub-subsection, save the content\n",
    "                elif text:                   \n",
    "                    estructura.setdefault(current_section, {}) \\\n",
    "                              .setdefault('content', []).append(text)\n",
    "\n",
    "    estructura_final = {final_name: estructura}\n",
    "\n",
    "    return estructura_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parseando el PDF: HI 108.pdf\n",
      "Parseando el PDF: HI 35.pdf\n",
      "Parseando el PDF: HI 36.pdf\n",
      "Parseando el PDF: HI 37.pdf\n",
      "Parseando el PDF: HI 38.pdf\n",
      "Parseando el PDF: HI 39.pdf\n",
      "Parseando el PDF: HI 40.pdf\n",
      "Parseando el PDF: HI 41.pdf\n",
      "Parseando el PDF: HI 42.pdf\n",
      "Parseando el PDF: HI 43.pdf\n",
      "Parseando el PDF: HI 44.pdf\n",
      "Parseando el PDF: HI 45.pdf\n"
     ]
    }
   ],
   "source": [
    "for pdf_file in os.listdir(raw_path / 'orientacion'):\n",
    "    #Create PDF Path\n",
    "    path = raw_path / 'orientacion' / pdf_file\n",
    "    print(f'Parseando el PDF: {pdf_file}')\n",
    "    \n",
    "    # Estructurar Autorizacion PDF\n",
    "    pdf_estructurado = estructurar_autorizaciones(path, path.name)\n",
    "\n",
    "    # Hacer un split para tener chunks de maximo 1000 tokens\n",
    "    chunks = []\n",
    "\n",
    "    for k,v in obtener_articulos_diccionario(documento=pdf_estructurado, prefijo='', articulos=None, temario=True).items():\n",
    "        enc = tiktoken.encoding_for_model(MODEL)\n",
    "        text_tokens = len(enc.encode(v[0]))\n",
    "\n",
    "        if text_tokens < 1000:\n",
    "            chunk_content = f'{k}: \\n {v[0]}'\n",
    "            chunks.append(chunk_content)\n",
    "        else:\n",
    "            text_chunks = chunks_text(text=v[0], max_tokens=1000, model=MODEL)\n",
    "            for chunk, chunk_idx in zip(text_chunks, range(len(text_chunks))):\n",
    "                chunk_content = f'{k} Parte {chunk_idx+1}: \\n {chunk}'\n",
    "                chunks.append(chunk_content)\n",
    "\n",
    "    # Create a json file with each chunk and context content\n",
    "    pdf_estructurado_split = {}\n",
    "\n",
    "    for i in range(len(chunks)):\n",
    "        pdf_estructurado_split[i] = {\n",
    "            'chunk_content': chunks[i]\n",
    "        }\n",
    "    # Create json path \n",
    "    path_start = Path(str(path.parent).replace('01_raw', '02_intermediate'))\n",
    "    #print(path_start)\n",
    "    json_name = path.name.replace(path.suffix, '.json')\n",
    "    #print(json_name)\n",
    "    pdf_estructurado_path = path_start / json_name\n",
    "    #print(pdf_estructurado_path)\n",
    "    save_json(pdf_estructurado_path, pdf_estructurado_split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackaton",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
